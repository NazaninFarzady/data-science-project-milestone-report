---
title: "Milestone Report"
output: html_document
date: '2022-07-11'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary of project

The aim of this assignment is getting better understanding of the various
statistical properties and predicting algorithm.

Using exploratory data analysis, this report describes the major features of the
training data and then summarizes next steps for creating the predictive model using Shiny application.

The model will be trained using a unified document corpus compiled from the
following three sources of text data:

1. Blogs
1. News
1. Twitter

The provided text data are provided in four different languages. This project
will only focus on the English corpora.

```{r}
getwd()
```

## Downloading and reading in files

```{r}
DataUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
DataFile <- "Newdata/Coursera-SwiftKey.zip"
if (!file.exists('Newdata')) {
    dir.create('Newdata')
}
if (!file.exists("Newdata/final/en_US")) {
    tempFile <- tempfile()
    download.file(DataUrl, tempFile)
    unzip(tempFile, exdir = "Newdata")
    unlink(tempFile)
}
```


```{r}
## Loading Data from (Blogs, News, Twitter) sources
blogs <- readLines("Newdata/final/en_US/en_US.blogs.txt", warn = FALSE, encoding = "UTF-8")
news <- readLines("Newdata/final/en_US/en_US.news.txt", warn = FALSE, encoding = "UTF-8")
twitter <- readLines("Newdata/final/en_US/en_US.twitter.txt", warn = FALSE, encoding = "UTF-8")
```


Basic summary of the three text corpora including the size of file, number of lines, number of characters, counting words. Then creating table to present information


```{r}
library(stringi) # stats files

# Size of Files file.size in Megabytes
Sblogs <- file.info("Newdata/final/en_US/en_US.blogs.txt")$size / 1024^2
Snews <- file.info("Newdata/final/en_US/en_US.news.txt")$size  / 1024^2 
Stwitter <- file.info("Newdata/final/en_US/en_US.twitter.txt")$size / 1024^2

# Number of Lines num.lines
Lblogs <- length(blogs)
Lnews <- length(news)
Ltwitter <- length(twitter)

# Number of characters
NCblogs <- sum(nchar(blogs))
NCnews <- sum(nchar(news))
NCtwitter <- sum(nchar(twitter))

# Counting the Words (num.words)
NWblogs <- sum(stri_count_words(blogs)) 
NWnews <- sum(stri_count_words(news))
NWtwitter <-sum(stri_count_words(twitter))

# create table 
data.frame(file.name = c("blogs", "news", "twitter"),
files.size.MB = c(Sblogs,Snews,Stwitter),
num.lines = c(Lblogs,Lnews,Ltwitter),
num.character = c(NCblogs,NCnews,NCtwitter),
num.words = c(NWblogs,NWnews,NWtwitter))
```

## Sampling

Remove all non-English characters and then compile a sample data set. Since the data set is too big 1% of each files is separated  by using sample(). 

```{r}
set.seed(123)
sample_blogs <-iconv(blogs,"latin1","ASCII",sub="")
sample_news <-iconv(news,"latin1","ASCII",sub="")
sample_twitter <-iconv(twitter,"latin1","ASCII",sub="")
# sample data set only 1% of each file
sample_data <-c(sample(sample_blogs,length(sample_blogs)*0.01),
               sample(sample_news,length(sample_news)*0.01),
               sample(sample_twitter,length(sample_twitter)*0.01))
```


## Clean and Build Corpus

```{r, echo=TRUE}
library(tm) # Text mining
library(NLP)
corpus <- VCorpus(VectorSource(sample_data))
Onecorpus <- tm_map(corpus,removePunctuation)
Twocorpus <- tm_map(Onecorpus,stripWhitespace)
Threecorpus <- tm_map(Twocorpus,tolower) # Convert to lowercase
Fourcorpus <- tm_map(Threecorpus,removeNumbers)
Fivecorpus <- tm_map(Fourcorpus,PlainTextDocument)
#removing stop words in English (a, as, at, so, etc.)
Sixcorpus <- tm_map(Fivecorpus,removeWords,stopwords("english")) 
```

## Build N-Grams 
In Natural Language Processing (NLP),  *n*-gram is a contiguous sequence of n items from a given sequence of text or speech. 

We next need to tokenize the clean Corpus (i.e., break the text up into words and short phrases) and construct a set of N-grams. We will start with the following three N-Grams:

Unigram - A matrix containing individual words

Bigram - A matrix containing two-word patterns

Trigram - A matrix containing three-word patterns

The following function is used to extract 1-grams, 2-grams, 3-grams from the text Corpus using RWeka.
Then, we calculate the frequencies of the N-Grams and see what these look like.

```{r}
install.packages('RWeka')
```

```{r, echo=TRUE}
library(RWeka)# tokenizer - create unigrams, bigrams, trigrams
#the RWeka package will construct functions that tokenize the sample and construct matrices of uniqrams, bigrams, and trigrams.
one<-function(x) NGramTokenizer(x,Weka_control(min=1,max=1))
two<-function(x) NGramTokenizer(x,Weka_control(min=2,max=2))
three<-function(x) NGramTokenizer(x,Weka_control(min=3,max=3))

tblOne<-TermDocumentMatrix(Sixcorpus,control=list(tokenize=one))
tblTwo<-TermDocumentMatrix(Sixcorpus,control=list(tokenize=two))
tblthree<-TermDocumentMatrix(Sixcorpus,control=list(tokenize=three))

#Then I find the frequency of terms in each of these 3 matrices and construct dataframes of these frequencies.
corpusOne<-findFreqTerms(tblOne,lowfreq=1000)
corpusTwo<-findFreqTerms(tblTwo,lowfreq=80)
corpusThree<-findFreqTerms(tblthree,lowfreq=10)
corpusOneNum<-rowSums(as.matrix(tblOne[corpusOne,]))
corpusOneTbl<-data.frame(Word=names(corpusOneNum),frequency=corpusOneNum)
corpusOneSort<-corpusOneTbl[order(-corpusOneTbl$frequency),]
head(corpusOneSort)

corpusTwoNum<-rowSums(as.matrix(tblTwo[corpusTwo,]))
corpusTwoTbl<-data.frame(Word=names(corpusTwoNum),frequency=corpusTwoNum)
corpusTwoSort<-corpusTwoTbl[order(-corpusTwoTbl$frequency),]
head(corpusTwoSort)

corpusThreeNum<-rowSums(as.matrix(tblthree[corpusThree,]))
corpusThreeTbl<-data.frame(Word=names(corpusThreeNum),frequency=corpusThreeNum)
corpusThreeSort<-corpusThreeTbl[order(-corpusThreeTbl$frequency),]
head(corpusThreeSort)
```

## Exploratory Analysis (Graphs & Visualizations)

The frequency distribution of each n-grams category were visualized into 3 different bar plots. 

```{r, echo=TRUE}
library(ggplot2) #visualization
plotOne<-ggplot(corpusOneSort[1:10,],aes(x=reorder(Word,-frequency),y=frequency,fill=frequency))
plotOne<-plotOne+geom_bar(stat="identity")
plotOne<-plotOne+labs(title="Unigrams",x="Words",y="Frequency")
plotOne<-plotOne+theme(axis.text.x=element_text(angle=90))
plotOne

plotTwo<-ggplot(corpusTwoSort[1:10,],aes(x=reorder(Word,-frequency),y=frequency,fill=frequency))
plotTwo<-plotTwo+geom_bar(stat="identity")
plotTwo<-plotTwo+labs(title="Bigrams",x="Words",y="Frequency")
plotTwo<-plotTwo+theme(axis.text.x=element_text(angle=90))
plotTwo

plotThree<-ggplot(corpusThreeSort[1:10,],aes(x=reorder(Word,-frequency),y=frequency,fill=frequency))
plotThree<-plotThree+geom_bar(stat="identity")
plotThree<-plotThree+labs(title="Trigrams",x="Words",y="Frequency")
plotThree<-plotThree+theme(axis.text.x=element_text(angle=90))
plotThree
```

## Conclusion & Next Steps

Because the data set is too big, in this assignment 1% of data has been considered as sample. Later due to memory consumption and getting higher performance, increasing the sample size might be the case. 


The next steps of this capstone project would be to create predictive models(s) based on the N-gram Tokenization, and deploy it as a data product. Here are my steps:

1. Establish the predictive model(s) by using N-gram Tokenizations.
2. Optimize the code for faster processing.
3. Develop data product, a Shiny App, to make a next word prediction based on user inputs.
4. Create a Slide Deck for pitching my algorithm and Shiny App.


